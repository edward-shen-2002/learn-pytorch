{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import Compose, Resize, ToTensor \n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download training and test data\n",
    "train_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=Compose([Resize(32), ToTensor()])\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=Compose([Resize(32), ToTensor()])\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dl = DataLoader(train_data, batch_size=batch_size)\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class VGG11(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Convolution layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv2d(1, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # Layer 2\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # Layer 3\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            # Layer 4\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # Layer 5\n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            # Layer 6\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # Layer 7\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            # Layer 8\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        # Flattener\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            # Layer 9\n",
    "            nn.Linear(512, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            # Layer 10\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            # Layer 11; final\n",
    "            nn.Linear(4096, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.368035  [   64/60000]\n",
      "loss: 2.004352  [ 6464/60000]\n",
      "loss: 1.680520  [12864/60000]\n",
      "loss: 1.294444  [19264/60000]\n",
      "loss: 0.868130  [25664/60000]\n",
      "loss: 0.655955  [32064/60000]\n",
      "loss: 0.364761  [38464/60000]\n",
      "loss: 0.336109  [44864/60000]\n",
      "loss: 0.334184  [51264/60000]\n",
      "loss: 0.303655  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 0.156593 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.146812 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.246662  [   64/60000]\n",
      "loss: 0.207480  [ 6464/60000]\n",
      "loss: 0.177055  [12864/60000]\n",
      "loss: 0.172872  [19264/60000]\n",
      "loss: 0.107401  [25664/60000]\n",
      "loss: 0.166205  [32064/60000]\n",
      "loss: 0.067316  [38464/60000]\n",
      "loss: 0.132390  [44864/60000]\n",
      "loss: 0.147848  [51264/60000]\n",
      "loss: 0.157896  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.065584 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.064515 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.086865  [   64/60000]\n",
      "loss: 0.132496  [ 6464/60000]\n",
      "loss: 0.091603  [12864/60000]\n",
      "loss: 0.079095  [19264/60000]\n",
      "loss: 0.050813  [25664/60000]\n",
      "loss: 0.089968  [32064/60000]\n",
      "loss: 0.047958  [38464/60000]\n",
      "loss: 0.048260  [44864/60000]\n",
      "loss: 0.130337  [51264/60000]\n",
      "loss: 0.163177  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.040778 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.044141 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.045881  [   64/60000]\n",
      "loss: 0.139406  [ 6464/60000]\n",
      "loss: 0.045736  [12864/60000]\n",
      "loss: 0.049368  [19264/60000]\n",
      "loss: 0.035554  [25664/60000]\n",
      "loss: 0.090368  [32064/60000]\n",
      "loss: 0.027901  [38464/60000]\n",
      "loss: 0.036709  [44864/60000]\n",
      "loss: 0.068417  [51264/60000]\n",
      "loss: 0.130092  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.028696 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.035562 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.022658  [   64/60000]\n",
      "loss: 0.097292  [ 6464/60000]\n",
      "loss: 0.037610  [12864/60000]\n",
      "loss: 0.040880  [19264/60000]\n",
      "loss: 0.016148  [25664/60000]\n",
      "loss: 0.050548  [32064/60000]\n",
      "loss: 0.039718  [38464/60000]\n",
      "loss: 0.020752  [44864/60000]\n",
      "loss: 0.087074  [51264/60000]\n",
      "loss: 0.119817  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 99.5%, Avg loss: 0.020980 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.030619 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model = VGG11().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 5\n",
    "train_loss = [0] * epochs\n",
    "test_loss = [0] * epochs\n",
    "train_acc = [0] * epochs\n",
    "test_acc = [0] * epochs\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dl, model, loss_fn, optimizer)\n",
    "    train_loss[t], train_acc[t] = test(train_dl, model, loss_fn)\n",
    "    test_loss[t], test_acc[t] = test(test_dl, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "torch.save(model.state_dict(), 'q3/model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm(image, epsilon, grad):\n",
    "    sign_grad = grad.sign()\n",
    "    perturbed_image = image + epsilon * sign_grad\n",
    "    return perturbed_image\n",
    "\n",
    "def adversarial_test(dataloader, model, loss_fn, adv_gen, epsilon):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    adv_samples = []\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        X.requires_grad = True\n",
    "        init_pred = model(X)\n",
    "        attack_loss = loss_fn(init_pred, y)\n",
    "        model.zero_grad()\n",
    "        attack_loss.backward()\n",
    "        grad = X.grad.data\n",
    "        adversarial_X = adv_gen(X, epsilon, grad)\n",
    "        if len(adv_samples) < 5:\n",
    "            adv_samples.append(adversarial_X)\n",
    "        pred = model(adversarial_X)\n",
    "        test_loss += loss_fn(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return adv_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.158134 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 10.8%, Avg loss: 5.048278 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 9.7%, Avg loss: 11.755159 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epsilon in (0.1, 0.2, 0.5):\n",
    "    adv_samples = adversarial_test(test_dl, model, loss_fn, fgsm, epsilon)\n",
    "    for i in range(len(adv_samples)):\n",
    "        x = adv_samples[i]\n",
    "        save_image(x.view(batch_size, 1, 32, 32), f'figs/q3p2-fgsm-{epsilon}-{i+1}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_train(dataloader, model, loss_fn, optimizer, adv_gen, epsilon):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        X.requires_grad = True\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        grad = X.grad.data\n",
    "        optimizer.step()\n",
    "        \n",
    "        # generate adversarial\n",
    "        adv_X = adv_gen(X, epsilon, grad)\n",
    "        adv_pred = model(adv_X)\n",
    "        adv_loss = loss_fn(adv_pred, y)\n",
    "\n",
    "        adv_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, adv_loss, current = loss.item(), adv_loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}, adversarial: {adv_loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.346556, adversarial: 2.329785  [   64/60000]\n",
      "loss: 1.356106, adversarial: 1.661337  [ 6464/60000]\n",
      "loss: 0.570784, adversarial: 1.007124  [12864/60000]\n",
      "loss: 0.349652, adversarial: 0.825833  [19264/60000]\n",
      "loss: 0.184833, adversarial: 0.687427  [25664/60000]\n",
      "loss: 0.177587, adversarial: 0.810080  [32064/60000]\n",
      "loss: 0.105211, adversarial: 0.585238  [38464/60000]\n",
      "loss: 0.124611, adversarial: 0.754988  [44864/60000]\n",
      "loss: 0.135895, adversarial: 0.650268  [51264/60000]\n",
      "loss: 0.149023, adversarial: 0.695961  [57664/60000]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.097780, adversarial: 0.413085  [   64/60000]\n",
      "loss: 0.107108, adversarial: 0.415530  [ 6464/60000]\n",
      "loss: 0.077144, adversarial: 0.405170  [12864/60000]\n",
      "loss: 0.073841, adversarial: 0.469090  [19264/60000]\n",
      "loss: 0.054380, adversarial: 0.393210  [25664/60000]\n",
      "loss: 0.062997, adversarial: 0.607207  [32064/60000]\n",
      "loss: 0.056725, adversarial: 0.246300  [38464/60000]\n",
      "loss: 0.067643, adversarial: 0.622264  [44864/60000]\n",
      "loss: 0.120802, adversarial: 0.504103  [51264/60000]\n",
      "loss: 0.103964, adversarial: 0.497131  [57664/60000]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.027860, adversarial: 0.350917  [   64/60000]\n",
      "loss: 0.083687, adversarial: 0.362098  [ 6464/60000]\n",
      "loss: 0.072272, adversarial: 0.329715  [12864/60000]\n",
      "loss: 0.059930, adversarial: 0.400523  [19264/60000]\n",
      "loss: 0.032966, adversarial: 0.301484  [25664/60000]\n",
      "loss: 0.037512, adversarial: 0.437164  [32064/60000]\n",
      "loss: 0.059492, adversarial: 0.273370  [38464/60000]\n",
      "loss: 0.029949, adversarial: 0.578737  [44864/60000]\n",
      "loss: 0.099613, adversarial: 0.366258  [51264/60000]\n",
      "loss: 0.064106, adversarial: 0.410995  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.226003 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.460145 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 22.3%, Avg loss: 2.104412 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = VGG11().to(device)\n",
    "epsilon = 0.2\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    adversarial_train(train_dl, model, loss_fn, optimizer, fgsm, epsilon)\n",
    "for epsilon in (0.1, 0.2, 0.5):\n",
    "    adversarial_test(test_dl, model, loss_fn, fgsm, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_IT = 20\n",
    "def pgd(image, epsilon, grad):\n",
    "    perturbed_image = image\n",
    "    for t in range(1, MAX_IT):\n",
    "        step = 1/t\n",
    "        perturbed_image = perturbed_image + step * grad\n",
    "        perturbed_image /= torch.max(perturbed_image) * epsilon\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [64, 1, 32, 32]], which is output 0 of DivBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\edwar\\uwat\\cs480\\a4\\cs480-a4\\a4q3.ipynb Cell 13\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/edwar/uwat/cs480/a4/cs480-a4/a4q3.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/edwar/uwat/cs480/a4/cs480-a4/a4q3.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/edwar/uwat/cs480/a4/cs480-a4/a4q3.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     adversarial_train(train_dl, model, loss_fn, optimizer, pgd, epsilon)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/edwar/uwat/cs480/a4/cs480-a4/a4q3.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m epsilon \u001b[39min\u001b[39;00m (\u001b[39m0.1\u001b[39m, \u001b[39m0.2\u001b[39m, \u001b[39m0.5\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/edwar/uwat/cs480/a4/cs480-a4/a4q3.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     adversarial_test(test_dl, model, loss_fn, fgsm, epsilon)\n",
      "\u001b[1;32mc:\\Users\\edwar\\uwat\\cs480\\a4\\cs480-a4\\a4q3.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/edwar/uwat/cs480/a4/cs480-a4/a4q3.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m adv_pred \u001b[39m=\u001b[39m model(adv_X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/edwar/uwat/cs480/a4/cs480-a4/a4q3.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m adv_loss \u001b[39m=\u001b[39m loss_fn(adv_pred, y)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/edwar/uwat/cs480/a4/cs480-a4/a4q3.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m adv_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/edwar/uwat/cs480/a4/cs480-a4/a4q3.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/edwar/uwat/cs480/a4/cs480-a4/a4q3.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mif\u001b[39;00m batch \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [64, 1, 32, 32]], which is output 0 of DivBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "model = VGG11().to(device)\n",
    "epsilon = 0.2\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    adversarial_train(train_dl, model, loss_fn, optimizer, pgd, epsilon)\n",
    "for epsilon in (0.1, 0.2, 0.5):\n",
    "    adversarial_test(test_dl, model, loss_fn, fgsm, epsilon)\n",
    "    adversarial_test(test_dl, model, loss_fn, pgd, epsilon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
